{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "reviews = [] # a list to hold all of the reviews\n",
    "\n",
    "f = gzip.open(\"reviews_Video_Games_5.json.gz\", \"rt\", encoding=\"utf-8\") #opens file   \n",
    "\n",
    "for line in f.readlines():#parse through json file creating json objects\n",
    "    y = json.loads(line) #creates json objects of each line in the file\n",
    "    reviews.append(y) #adds that json object to the list\n",
    "    \n",
    "#testing github-vonterio branch       \n",
    "#print(reviews.pop()) #prints the top json object just for testing if everything works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalList = [] #list to hold tuples (product#, review text, overall rating)\n",
    "\n",
    "#iterate through each review object\n",
    "for review in reviews:\n",
    "    \n",
    "    pairs = review.items() #gets each individual pair in reviews\n",
    "    for key,value in pairs:\n",
    "        \n",
    "        #If key is product num, review text, or overall rating assign value\n",
    "        if (key == \"asin\"):\n",
    "            productID = value\n",
    "            \n",
    "        elif (key == \"reviewText\"):\n",
    "            text = value\n",
    "        \n",
    "        elif (key == \"overall\"):\n",
    "            rating = value\n",
    "        \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    #Append tuple to final list\n",
    "    finalList.append((productID, text, rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0700099867', 'Installing the game was a struggle (because of games for windows live bugs).Some championship races and cars can only be \"unlocked\" by buying them as an addon to the game. I paid nearly 30 dollars when the game was new. I don\\'t like the idea that I have to keep paying to keep playing.I noticed no improvement in the physics or graphics compared to Dirt 2.I tossed it in the garbage and vowed never to buy another codemasters game. I\\'m really tired of arcade style rally/racing games anyway.I\\'ll continue to get my fix from Richard Burns Rally, and you should to. :)http://www.amazon.com/Richard-Burns-Rally-PC/dp/B000C97156/ref=sr_1_1?ie=UTF8&qid;=1341886844&sr;=8-1&keywords;=richard+burns+rallyThank you for reading my review! If you enjoyed it, be sure to rate it as helpful.', 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(finalList[0]) #Testing final list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict #necessary import for what we need to do\n",
    "\n",
    "condensedList = defaultdict(list) \n",
    "\n",
    "#Iterate and get separate parts of eaach tuple\n",
    "for productID, review, rating in finalList:\n",
    "    condensedList[productID].append((review,rating)) #Uses ID as a key and appends to new list, if key exists it adds on\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(condensedList[\"0700099867\"]) #Testing condensed list, ID will show all reviews and ratings in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Vonterio\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_sentiment(sentence):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    \n",
    "    nltk_sentiment = SentimentIntensityAnalyzer()\n",
    "    score = nltk_sentiment.polarity_scores(sentence)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsForProduct = []\n",
    "\n",
    "#Add reviews for product into the list\n",
    "for key, value in condensedList.items(): #outer loop gets productID, [(review, rating),(review, rating)...]\n",
    "    for review, rating in value: #inner loop gets (review,rating)\n",
    "        reviewsForProduct.append(review) #Append single review to reviewsForProduct\n",
    "        #add up ratings\n",
    "\n",
    "    #Do analysis of reviews here, get average of ratings\n",
    "    #Add a final score to product after combining both review score / rating score\n",
    "    #Add final score to a dictionary paired with productID\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code from website\n",
    "nltk_results = [nltk_sentiment(row) for row in reviewsForProduct] #This line alone will print the results from each review\n",
    "\n",
    "#This is for if we want to export results to excel or google sheets for cleaner formatting\n",
    "results_df = pd.DataFrame(nltk_results)\n",
    "text_df = pd.DataFrame(reviewsForProduct, columns = ['text'])\n",
    "nltk_df = text_df.join(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'neg': 0.093, 'neu': 0.795, 'pos': 0.112, 'compound': 0.525}, {'neg': 0.089, 'neu': 0.799, 'pos': 0.111, 'compound': 0.308}, {'neg': 0.18, 'neu': 0.769, 'pos': 0.051, 'compound': -0.8139}, {'neg': 0.076, 'neu': 0.768, 'pos': 0.156, 'compound': 0.9955}, {'neg': 0.066, 'neu': 0.67, 'pos': 0.264, 'compound': 0.9265}, {'neg': 0.115, 'neu': 0.672, 'pos': 0.213, 'compound': 0.9241}, {'neg': 0.122, 'neu': 0.618, 'pos': 0.26, 'compound': 0.9286}, {'neg': 0.035, 'neu': 0.817, 'pos': 0.148, 'compound': 0.9022}, {'neg': 0.042, 'neu': 0.797, 'pos': 0.161, 'compound': 0.8746}, {'neg': 0.223, 'neu': 0.626, 'pos': 0.151, 'compound': -0.7955}, {'neg': 0.152, 'neu': 0.635, 'pos': 0.213, 'compound': 0.9831}, {'neg': 0.085, 'neu': 0.704, 'pos': 0.211, 'compound': 0.9869}, {'neg': 0.13, 'neu': 0.87, 'pos': 0.0, 'compound': -0.34}, {'neg': 0.192, 'neu': 0.709, 'pos': 0.099, 'compound': -0.5713}, {'neg': 0.089, 'neu': 0.911, 'pos': 0.0, 'compound': -0.3089}, {'neg': 0.128, 'neu': 0.872, 'pos': 0.0, 'compound': -0.4219}, {'neg': 0.125, 'neu': 0.732, 'pos': 0.142, 'compound': 0.8899}, {'neg': 0.141, 'neu': 0.818, 'pos': 0.041, 'compound': -0.9369}, {'neg': 0.028, 'neu': 0.912, 'pos': 0.06, 'compound': 0.4926}, {'neg': 0.118, 'neu': 0.774, 'pos': 0.108, 'compound': -0.713}, {'neg': 0.217, 'neu': 0.626, 'pos': 0.158, 'compound': -0.3865}, {'neg': 0.112, 'neu': 0.668, 'pos': 0.221, 'compound': 0.9426}, {'neg': 0.285, 'neu': 0.57, 'pos': 0.145, 'compound': -0.7845}, {'neg': 0.036, 'neu': 0.767, 'pos': 0.197, 'compound': 0.9824}, {'neg': 0.036, 'neu': 0.862, 'pos': 0.103, 'compound': 0.9272}]\n"
     ]
    }
   ],
   "source": [
    "print(nltk_results) #printing results - as of now only results for productID 0700099867"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
